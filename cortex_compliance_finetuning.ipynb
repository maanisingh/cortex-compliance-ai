{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cortex Compliance AI - Fine-Tuning\n\nDataset already uploaded to HuggingFace. Just run this notebook on Colab with T4 GPU.\n\n## Steps:\n1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n2. Run all cells"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install Unsloth (2x faster, handles all dependencies)\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps trl peft accelerate bitsandbytes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load dataset from HuggingFace (already uploaded!)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"maaninder/cortex-compliance-data\", split=\"train\")\nprint(f\"‚úÖ Loaded {len(dataset)} training examples from HuggingFace\")\nprint(dataset[0][\"text\"][:200] + \"...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Load model with Unsloth (handles 4-bit quantization automatically)\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",  # Pre-quantized!\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n)\nprint(\"‚úÖ Model loaded with LoRA\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Train with SFTTrainer\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=TrainingArguments(\n        output_dir=\"./cortex-compliance-ai\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        save_strategy=\"epoch\",\n        report_to=\"none\",\n    ),\n)\n\nprint(\"üöÄ Starting training...\")\ntrainer.train()\nprint(\"‚úÖ Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Save to HuggingFace Hub\nfrom huggingface_hub import notebook_login\nnotebook_login()  # Enter your token when prompted\n\nmodel.push_to_hub(\"maaninder/cortex-compliance-ai\")\ntokenizer.push_to_hub(\"maaninder/cortex-compliance-ai\")\nprint(\"‚úÖ Model saved to: https://huggingface.co/maaninder/cortex-compliance-ai\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Test the model\nFastLanguageModel.for_inference(model)\n\ntest_prompt = \"### Instruction:\\nGenerate a Personal Data Processing Policy for –û–û–û –¢–µ—Å—Ç (INN: 1234567890)\\n\\n### Response:\\n\"\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nModel: https://huggingface.co/maaninder/cortex-compliance-ai\n\n**Using Unsloth benefits:**\n- 2x faster training\n- No triton/bitsandbytes errors\n- Pre-quantized models\n- Works reliably on Colab T4"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}