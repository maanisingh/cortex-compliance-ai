{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cortex Compliance AI - Fine-Tuning with Phi-2\n\nUses Microsoft Phi-2 (2.7B) - small enough to train without quantization.\n\n**No bitsandbytes/triton issues!**\n\n## Steps:\n1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n2. Run all cells"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies (simple - no bitsandbytes needed)\n!pip install -q transformers==4.41.0 accelerate peft trl datasets huggingface_hub sentencepiece\n\nprint(\"‚úÖ Installed! Run all cells (no restart needed)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load dataset from HuggingFace (already uploaded!)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"maaninder/cortex-compliance-data\", split=\"train\")\nprint(f\"‚úÖ Loaded {len(dataset)} training examples from HuggingFace\")\nprint(dataset[0][\"text\"][:200] + \"...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Load Phi-2 model (2.7B - fits on T4 without quantization)\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nMODEL_NAME = \"microsoft/phi-2\"  # 2.7B params, fits on T4 GPU\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Add LoRA\nmodel = get_peft_model(model, LoraConfig(\n    r=16, lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n))\n\nprint(\"‚úÖ Phi-2 loaded with LoRA\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Train\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=1024,\n    args=TrainingArguments(\n        output_dir=\"./output\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"no\",\n        report_to=\"none\",\n    ),\n)\n\nprint(\"üöÄ Training started...\")\ntrainer.train()\nprint(\"‚úÖ Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Save to HuggingFace Hub\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nmodel.push_to_hub(\"maaninder/cortex-compliance-phi2\")\ntokenizer.push_to_hub(\"maaninder/cortex-compliance-phi2\")\nprint(\"‚úÖ Model saved to: https://huggingface.co/maaninder/cortex-compliance-phi2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Test the model\ntest_prompt = \"### Instruction:\\nGenerate a Personal Data Processing Policy for –û–û–û –¢–µ—Å—Ç (INN: 1234567890)\\n\\n### Response:\\n\"\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\noutputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, do_sample=True)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nModel: https://huggingface.co/maaninder/cortex-compliance-phi2\n\n**Why Phi-2:**\n- 2.7B params (fits on T4 without quantization)\n- No bitsandbytes/triton needed\n- Still very capable for document generation"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}