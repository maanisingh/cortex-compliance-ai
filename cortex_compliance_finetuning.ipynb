{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cortex Compliance AI - Fine-Tuning Notebook\n\nFine-tunes Mistral-7B for Russian compliance document generation.\n\n## Requirements:\n- **Google Colab (Recommended)**: Free T4 GPU, pre-configured\n- **Local**: NVIDIA GPU with 8GB+ VRAM, CUDA 11.8+, Linux recommended\n\n## Quick Start (Colab):\n1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n2. Run Step 1, then Runtime ‚Üí Restart runtime\n3. Run Steps 2-9\n\n## Why Colab?\n- 217 training examples is small but sufficient for LoRA\n- Fine-tuning 7B model needs ~10GB VRAM (even quantized)\n- Colab provides free GPU access"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies\n# After running this cell: Runtime ‚Üí Restart runtime, then run from Step 2\n\nimport subprocess\nimport sys\n\n# Uninstall conflicting versions quietly\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"transformers\", \"accelerate\", \"peft\", \"bitsandbytes\", \"triton\"], \n               capture_output=True)\n\n# Install compatible versions + triton for GPU optimization\n!pip install -q transformers==4.44.0 accelerate==0.33.0 peft==0.12.0 bitsandbytes==0.43.1\n!pip install -q datasets huggingface_hub trl sentencepiece protobuf\n!pip install -q triton  # GPU optimization\n\nprint(\"\\n‚úÖ Packages installed!\")\nprint(\"üëâ Now go to: Runtime ‚Üí Restart runtime\")\nprint(\"üëâ Then run from Step 2 (skip this cell)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load Training Data - 217 Russian Business Document Templates\n# Includes: Contracts, Corporate Docs, Financial, HR, Legal, Tax, Industry, Specialized and more!\n\nimport json\n\n# Download training data from GitHub (217 examples from 265 templates)\n!wget -q https://raw.githubusercontent.com/maanisingh/cortex-compliance-ai/main/combined_training_data.jsonl -O training_data.jsonl\n\n# Load training data\nTRAINING_DATA = []\nwith open('training_data.jsonl', 'r') as f:\n    for line in f:\n        TRAINING_DATA.append(json.loads(line))\n\nprint(f\"Loaded {len(TRAINING_DATA)} training examples\")\nprint(f\"\\nSample categories:\")\nfor i, item in enumerate(TRAINING_DATA[:5]):\n    print(f\"  {i+1}. {item['instruction'][:60]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n",
    "\n",
    "dataset = Dataset.from_list(TRAINING_DATA)\n",
    "dataset = dataset.map(format_prompt)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Load model with 4-bit quantization\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nprint(f\"‚úÖ Model loaded: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Configure LoRA\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Train using Trainer\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./cortex-compliance-ai\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    warmup_steps=10,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n\nprint(\"üöÄ Starting training...\")\ntrainer.train()\nprint(\"‚úÖ Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Save to Hugging Face Hub\nfrom huggingface_hub import whoami\n\n# Get your HuggingFace username automatically\nhf_user = whoami()[\"name\"]\nMODEL_REPO = f\"{hf_user}/cortex-compliance-ai\"\n\nprint(f\"Uploading to: {MODEL_REPO}\")\nmodel.push_to_hub(MODEL_REPO)\ntokenizer.push_to_hub(MODEL_REPO)\nprint(f\"Model saved to: https://huggingface.co/{MODEL_REPO}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Test the model\n",
    "test_prompt = \"### Instruction:\\nGenerate a Personal Data Processing Policy for –û–û–û –¢–µ—Å—Ç (INN: 1234567890)\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nYour model is now available at: https://huggingface.co/maaninder/cortex-compliance-ai\n\nThe Cortex GRC backend is configured to use this model for AI document generation."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}