{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortex Compliance AI - Fine-Tuning Notebook\n",
    "\n",
    "Fine-tunes Mistral-7B for Russian compliance document generation.\n",
    "\n",
    "## Quick Start:\n",
    "1. Runtime → Change runtime type → **T4 GPU**\n",
    "2. Run all cells\n",
    "3. Enter your HuggingFace token when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies (fixed compatible versions)\n!pip install -q transformers==4.44.0 accelerate==0.33.0 peft==0.12.0 bitsandbytes==0.43.1 datasets huggingface_hub trl\n!pip install -q sentencepiece protobuf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load Training Data - 217 Russian Business Document Templates\n# Includes: Contracts, Corporate Docs, Financial, HR, Legal, Tax, Industry, Specialized and more!\n\nimport json\n\n# Download training data from GitHub (217 examples from 265 templates)\n!wget -q https://raw.githubusercontent.com/maanisingh/cortex-compliance-ai/main/combined_training_data.jsonl -O training_data.jsonl\n\n# Load training data\nTRAINING_DATA = []\nwith open('training_data.jsonl', 'r') as f:\n    for line in f:\n        TRAINING_DATA.append(json.loads(line))\n\nprint(f\"Loaded {len(TRAINING_DATA)} training examples\")\nprint(f\"\\nSample categories:\")\nfor i, item in enumerate(TRAINING_DATA[:5]):\n    print(f\"  {i+1}. {item['instruction'][:60]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n",
    "\n",
    "dataset = Dataset.from_list(TRAINING_DATA)\n",
    "dataset = dataset.map(format_prompt)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Load model with 4-bit quantization\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# Using Mistral-7B-Instruct for better instruction following\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nprint(f\"Model loaded: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Configure LoRA\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train using Trainer (compatible with all versions)\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cortex-compliance-ai\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Save to Hugging Face Hub\nfrom huggingface_hub import whoami\n\n# Get your HuggingFace username automatically\nhf_user = whoami()[\"name\"]\nMODEL_REPO = f\"{hf_user}/cortex-compliance-ai\"\n\nprint(f\"Uploading to: {MODEL_REPO}\")\nmodel.push_to_hub(MODEL_REPO)\ntokenizer.push_to_hub(MODEL_REPO)\nprint(f\"Model saved to: https://huggingface.co/{MODEL_REPO}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Test the model\n",
    "test_prompt = \"### Instruction:\\nGenerate a Personal Data Processing Policy for ООО Тест (INN: 1234567890)\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nYour model is now available at: https://huggingface.co/maaninder/cortex-compliance-ai\n\nThe Cortex GRC backend is configured to use this model for AI document generation."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}