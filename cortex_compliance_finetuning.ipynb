{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cortex Compliance AI - Fine-Tuning with TinyLlama\n\nUses TinyLlama (1.1B) - simple setup, no complex dependencies.\n\n## Steps:\n1. Runtime ‚Üí **T4 GPU**\n2. Run all cells in order"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Clean install\n!pip uninstall -y transformers bitsandbytes peft accelerate -q 2>/dev/null\n!pip install -q transformers accelerate datasets huggingface_hub\n\nprint(\"‚úÖ Installed! Now: Runtime ‚Üí Restart runtime ‚Üí Run from Step 2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load dataset from HuggingFace (already uploaded!)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"maaninder/cortex-compliance-data\", split=\"train\")\nprint(f\"‚úÖ Loaded {len(dataset)} training examples from HuggingFace\")\nprint(dataset[0][\"text\"][:200] + \"...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Load TinyLlama\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,  # Use float32 for stable training\n    device_map=\"auto\",\n)\n\n# Freeze most layers, train last 2\nfor param in model.parameters():\n    param.requires_grad = False\nfor param in model.model.layers[-2:].parameters():\n    param.requires_grad = True\nfor param in model.lm_head.parameters():\n    param.requires_grad = True\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"‚úÖ TinyLlama loaded ({trainable:,} trainable params)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Train\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n\ntokenized = dataset.map(tokenize, remove_columns=[\"text\"])\n\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=\"./output\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        learning_rate=5e-5,\n        fp16=False,  # Disable fp16 to avoid gradient issues\n        logging_steps=10,\n        save_strategy=\"no\",\n        report_to=\"none\",\n        optim=\"adamw_torch\",\n    ),\n    train_dataset=tokenized,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nprint(\"üöÄ Training...\")\ntrainer.train()\nprint(\"‚úÖ Done!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Save to HuggingFace\nfrom huggingface_hub import notebook_login, HfApi\nnotebook_login()\n\nREPO = \"maaninder/cortex-compliance-tinyllama\"\nmodel.push_to_hub(REPO)\ntokenizer.push_to_hub(REPO)\nprint(f\"‚úÖ Saved to: https://huggingface.co/{REPO}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Test\nprompt = \"### Instruction:\\nGenerate a contract for –û–û–û –¢–µ—Å—Ç\\n\\n### Response:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    out = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nModel: https://huggingface.co/maaninder/cortex-compliance-tinyllama\n\n**Simple approach:**\n- TinyLlama 1.1B (no quantization needed)\n- No bitsandbytes/peft/triton\n- Just freeze layers + fine-tune"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}