{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cortex Compliance AI - Fine-Tuning\n\nDataset already uploaded to HuggingFace. Just run this notebook on Colab with T4 GPU.\n\n## Steps:\n1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n2. Run all cells"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies (tested stable versions)\n!pip uninstall -y transformers accelerate peft bitsandbytes trl -q 2>/dev/null\n!pip install -q transformers==4.41.0 accelerate==0.30.1 peft==0.11.1 bitsandbytes==0.43.1 trl==0.8.6\n!pip install -q datasets huggingface_hub sentencepiece\n\nprint(\"‚úÖ Installed! Now: Runtime ‚Üí Restart runtime ‚Üí Run from Step 2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load dataset from HuggingFace (already uploaded!)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"maaninder/cortex-compliance-data\", split=\"train\")\nprint(f\"‚úÖ Loaded {len(dataset)} training examples from HuggingFace\")\nprint(dataset[0][\"text\"][:200] + \"...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Load model with 4-bit quantization\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, LoraConfig(\n    r=16, lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n))\n\nprint(\"‚úÖ Model loaded with LoRA\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Train\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=1024,\n    args=TrainingArguments(\n        output_dir=\"./output\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"no\",\n        report_to=\"none\",\n    ),\n)\n\nprint(\"üöÄ Training started...\")\ntrainer.train()\nprint(\"‚úÖ Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Save to HuggingFace Hub\nfrom huggingface_hub import notebook_login\nnotebook_login()  # Enter your token when prompted\n\nmodel.push_to_hub(\"maaninder/cortex-compliance-ai\")\ntokenizer.push_to_hub(\"maaninder/cortex-compliance-ai\")\nprint(\"‚úÖ Model saved to: https://huggingface.co/maaninder/cortex-compliance-ai\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Test the model\ntest_prompt = \"### Instruction:\\nGenerate a Personal Data Processing Policy for –û–û–û –¢–µ—Å—Ç (INN: 1234567890)\\n\\n### Response:\\n\"\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\noutputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, do_sample=True)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nModel saved to: https://huggingface.co/maaninder/cortex-compliance-ai\n\n**Tested versions:**\n- transformers==4.41.0\n- peft==0.11.1\n- bitsandbytes==0.43.1\n- trl==0.8.6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}